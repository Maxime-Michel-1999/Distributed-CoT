{
    "mixtral-8x7b-32768": {
        "parameters": 12.9,
        "context_size": 32768
    },
    "llama3-8b-8192": {
        "parameters": 8,
        "context_size": 8192
    },
    "llama3-70b-8192": {
        "parameters": 70,
        "context_size": 8192
    },
    "gpt-4": {
        "parameters": 1750,
        "context_size": 8192
    }
}